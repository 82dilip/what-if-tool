<!DOCTYPE html>
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-46457317-11"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-46457317-11');
  </script>
  <title>WIT | Walkthrough</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://code.getmdl.io/1.3.0/material.indigo-deep_orange.min.css">
  <link href='http://fonts.googleapis.com/css?family=Roboto:300,300italic,400,400italic,500,500italic,700,700italic' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Noto+Serif:400,400i,700,700i" rel="stylesheet" type='text/css'>
  <script defer src="https://code.getmdl.io/1.3.0/material.min.js"></script>
  <link rel="icon" href="./assets/favicon.png" type="image/png"/>
  <style type="text/css">

  body {
    font-family: 'Roboto', sans-serif;
  }

  .page-content{
    max-width: 1600px;
    margin: 0 auto;
  }

  .hero-banner {
    height: 25vw;
    background-image:  url("./assets/wit-walkthrough.jpg");
    background-position: top;
    background-size: contain;
    margin-bottom: 12px;
    border-bottom: solid 1px #DADCE0;
  }

  .emphasis-italic {
    font-style: italic;
  }

  .uppercase {
    text-transform: uppercase;
  }

  img {
    border: 1px solid #DADCE0;
    width: 600px;
    height: auto;
    margin: 36px;
  }

  .para {
    margin-top: 12px;
    margin-bottom: 24px;
    }

/*  .bullet1 {
    padding-left: 8px;
  }

  .bullet2 {
    padding-left: 16px;
    }*/

    .bold {
      font-weight: 700;
    }

    .group-type{
      color: #db7205;
    }

    .quote {
      margin: 48px;
      padding: 24px;
      border-left: solid 2px #e57373;
    }

    .article-content {
      margin-bottom: 112px;
    }

    .separator {
      width: 40%;
      margin: 24px auto;
    }

    .learn {
      color: #202124;
      border-bottom: solid 4px #CBF0F8;
      padding-left: 6px;
      padding-right: 6px;
    }

    /*inline and citation or reference links*/
    .inline {
      background: rgba(255,179,68,0.3);
      color: #3C4043;
      padding: 2px 2px 2px 4px;
      font-weight: inherit;
      text-decoration: none;
    }

    .inline:hover {
      color: #283593;
      background: rgba(255,179,68,1);
    }

    /*Typography*/
    .mdl-typography--body-1 {
      font-size:  18px;
      color: #3C4043;
      font-family: 'Roboto', sans-serif;
      line-height: 1.8em;
    }

    .mdl-typography--body-2 {
      font-size:  18px;
      font-family: 'Roboto', sans-serif;
      line-height: 1.8em;
      color: #3C4043;
    }

    .mdl-typography--quote {
      font-family: 'Noto Serif', serif;
      font-size:  20px;
      font-style: italic;
      color: #c54a4a;
      line-height: 1.6em;
    }

    .mdl-typography--headline {
      font-size: 22px;
      font-weight: 300;
      margin-bottom: 12px;
      line-height: 1.6em;
    }

    .mdl-typography--title {
      margin-bottom: 12px;
      line-height: 1.6em;
    }

    .mdl-typography--display-3 {
      margin: 56px 0 0 0;
      color: #c54a4a;
    }

    .mdl-typography--display-1 {
      margin: 24px auto;
      color: #3C4043;
      font-weight: 300;
      line-height: 1.8em;
    }
    /*sections in text*/

    .author-note {
      font-family: 'Noto Serif', serif;
      font-size:  18px;
      font-style: italic;
      /*color: #1e348e;*/
      color: #283593;
      line-height: 1.8em;
      margin:24px auto;
    }

    /*Icons in Navigation, do not touch*/
    .icon-style {
      margin: auto 4px;
      font-size: 1.2em;
    }
    .button-style {
      margin: auto 6px;
    }
    .button-style > .icon-style {
      margin: auto 4px auto 8px;
      font-size: 1.6em;
    }

  </style>
</head>

<body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
    <header class="mdl-layout__header">
      <div class="mdl-layout__header-row">
        <!-- Title -->
        <span class="mdl-layout__title">What-If Tool</span>
        <!-- Add spacer, to align navigation to the right -->
        <div class="mdl-layout-spacer"></div>
        <!-- Navigation. We hide it in small screens. -->
        <nav class="mdl-navigation mdl-layout--large-screen-only">
          <a class="mdl-navigation__link" href="./index.html">Home</a>
          <a class="mdl-navigation__link" href="./index.html#intro">Introduction</a>
          <a class="mdl-navigation__link" href="./index.html#features">Features</a>
          <a class="mdl-navigation__link" href="./index.html#demos">Demos</a>
          <a class="mdl-navigation__link" href="./index.html#about">About</a>
          <a class="mdl-navigation__link" href="./index.html#references">References</a>
          <a class="mdl-navigation__link" href="./walkthrough.html">Walkthrough</a>
          <a class="mdl-navigation__link" href="./ai-fairness.html">AI Fairness</a>
          <a class="mdl-navigation__link button-style mdl-typography--text-uppercase" href="https://ai.google/research/teams/brain/pair" target="-_blank">
            <i class="material-icons icon-style">open_in_new</i>
            <span>PAIR</span>
          </a>
          <a class="mdl-navigation__link button-style mdl-typography--text-uppercase" href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/interactive_inference" target="-_blank">
            <i class="material-icons icon-style">link</i>
            <span>Github</span>
          </a>
        </nav>
      </div>
    </header>
    <div class="mdl-layout__drawer">
      <span class="mdl-layout-title">What If Tool</span>
      <nav class="mdl-navigation">
        <a class="mdl-navigation__link" href="./index.html">Home</a>
        <a class="mdl-navigation__link" href="./index.html#intro">Introduction</a>
        <a class="mdl-navigation__link" href="./index.html#features">Features</a>
        <a class="mdl-navigation__link" href="./index.html#demos">Demos</a>
        <a class="mdl-navigation__link" href="./index.html#about">About</a>
        <a class="mdl-navigation__link" href="./index.html#references">References</a>
        <a class="mdl-navigation__link" href="./walkthrough.html">Walkthrough</a>
        <a class="mdl-navigation__link" href="./ai-fairness.html">AI Fairness</a>
        <hr>
        <a class="mdl-navigation__link" href="https://ai.google/research/teams/brain/pair">About PAIR</a>
        <a class="mdl-navigation__link" href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/interactive_inference">Github</a>
        <a class="mdl-navigation__link" href="https://pair-code.github.io/facets/" target="-_blank">
          <i class="material-icons icon-style">star</i>
          <span>Facets</span>
        </a>
        <a class="mdl-navigation__link" href="https://js.tensorflow.org" target="-_blank">
          <i class="material-icons icon-style">star</i>
          <span>TensorFlow.js</span>
        </a>
      </nav>
    </div>

    <!-- Main page starts here -->
    <main class="mdl-layout__content">
      <div class="hero-banner">
      </div>
      <div class="page-content">
        <div class="mdl-grid">
          <div class="mdl-cell mdl-cell--6-col mdl-cell--2-offset article-title">
            <div class="article-headline mdl-typography--display-3">A Walkthrough of the What-If Tool</div>
          </div>
        </div>
      </div>

      <div class="mdl-grid article-content">
        <div class="mdl-cell mdl-cell--8-col mdl-cell--2-offset mdl-cell--8-col-tablet mdl-cell--4-col-phone">
          <div class="mdl-typography--headline para">In this walkthrough, we explore how the What-If Tool (WIT) can help us learn about a model and dataset.
            <!--             A walk through of an exploration of a model on test data using the What-If Tool (WIT), exploring the features of the tool and how it can help us learn about a model and dataset. --></div><hr>
            <div class="mdl-typography--display-1 section-title">Our Dataset and Model</div>
            <div class="mdl-typography--body-1 para">The <a class ="inline" href="https://archive.ics.uci.edu/ml/datasets/census+income">UCI Census</a> dataset is a dataset in which each record represents a person. Each record contains 14 pieces of census information about a single person, from the 1994 US census database. This includes information such as age, marital status and education level. The prediction task is to determine whether a person is high income (defined as earning more than $50k a year). This specific dataset and prediction task combination is often used in machine learning modeling and fairness research, partially due to the dataset's understandable attributes â€“ including some containing sensitive classes such as age and gender, and partially due to a clear, real-world prediction task.</div>

            <div class="mdl-typography--body-1 para">Using a training set of ~30,000 records, we've trained a simple <a class="inline" href="https://en.wikipedia.org/wiki/Linear_classifier">linear classifier</a> for this <a class="inline" href="https://en.wikipedia.org/wiki/Binary_classification">binary classification</a> task. For any datapoint that we provide it, the model will return a score between 0 and 1 for how confident it is that the person represented by the datapoint is high income. 1 means that the model is entirely confident that the datapoint falls in the high income category, and 0 means that it is not at all confident</div>

            <div class="mdl-typography--body-1 para">Now let's explore how the trained model performs on 5,000 records from a <a class="inline" href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Test_dataset">test set</a> that we held out. At each point, we'll highlight statements about the things WIT helped us learn about our dataset and trained model in <span class="learn">blue</span>.</div>

            <div class="mdl-typography--display-1 section-title ">Setting up WIT</div>
            <div class="mdl-typography--body-1 para">WIT can be used inside a <a class="inline" href="https://jupyter.org/">Jupyter</a> or <a class="inline" href="https://colab.research.google.com">Colab</a> notebook, or inside the <a class="inline" href="https://www.tensorflow.org/tensorboard">TensorBoard</a> web application.</div>

            <div class="mdl-typography--title section-subhead">WIT in Notebooks</div>

            <div class="mdl-typography--body-1 para">To use WIT inside a notebook, create a WitConfigBuilder object that specifies the data and model to be analyzed. Then, provide this object to the WitWidget object. This <a class="inline" href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/interactive_inference#notebook-mode-details">documentation</a> provides a step-by-step outline for using WIT in a notebook. Follow along this walkthrough using <a class="inline" href="https://colab.sandbox.google.com/github/tensorflow/tensorboard/blob/master/tensorboard/plugins/interactive_inference/What_If_Tool_Notebook_Usage.ipynb">this colab notebook</a> in which we train a UCI census model and visualize it on the test set.</div>
            <div class="image-container"><img src="./assets/wit-notebook-code.png" class="image"></div>

            <div class="mdl-typography--title section-subhead">WIT in TensorBoard</div>
            
            <div class="mdl-typography--body-1 para">To use WIT within TensorBoard, your model needs to be served through a <a class="inline" href="https://www.tensorflow.org/serving">TensorFlow Model Server</a>, and the data to be analyze must be available on disk as a <a class="inline" href="https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564">TFRecords</a> file. Navigate to the web address of your TensorBoard, select "What-If Tool" from TensorBoard's dashboard selector. Fill out the path to the TFRecords file, the host/port of the running TensorFlow Model Server, and the name of the model on that model server, that you wish to test. Click "Accept". For more details, refer to the <a class="inline" href="https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/interactive_inference#what-do-i-need-to-use-it-in-tensorboard">documentation</a> for using WIT in TensorBoard.</div>

            <div class="image-container"><img src="./assets/wit-tb-dialog.png" class="image"></div>
            
            <div class="mdl-typography--display-1 section-title"> Initial Views</div>
            
            <div class="mdl-typography--body-1 para">Once we've pointed the What-If Tool to our model and dataset, the first thing we see is the dataset visualized as individual points in <a class="inline" href="https://pair-code.github.io/facets/">Facets Dive</a>. Each datapoint is colored by the category that the model predicted for it, i.e. the inference label". Blue points represent people that the model inferred are high income and red points represent those whom the model inferred are low income.  Additionally, the points are laid out top to bottom by a score for how confident the model is that the person is high income, called "inference score". By default, WIT uses a <a class="inline" href="https://developers.google.com/machine-learning/crash-course/classification/thresholding">positive classification threshold</a> of 0.5. This means that if the inference score is 0.5 or more, the datapoint is considered to be in the positive class, i.e. high income. As a result, points in the top half of the visualization are blue whereas those on the bottom half are red. We will show later in this tutorial how to change this threshold. The settings that created this view are visible in the top control bar (see controls for "color by" and "scatter on Y-Axis by").</div>
            <div class="image-container"><img src="./assets/wit-initialview.png" class="image"></div>
            <div class="mdl-typography--body-1 para">What can we learn from this initial view? There are definitely more red points than blue points, which means that <span class="learn">the model predicts more people as low income than high income</span>. A lot of points are bunched at both the bottom and top of the visualization, which means that <span class="learn">our model is often very confident that a person is low income or high income</span>.</div><!-- jwexler@ - Is it safe to say that the model is either very confident or has very little confidence? -->

            <div class="mdl-typography--display-1 section-title"> Simple visual analyses</div>

            <div class="mdl-typography--body-1 para">Let's add some more data to this chart. We can set the x-axis scatter to a feature of the dataset, such as education level. In our dataset, this is described as a number that represents the last school year that a person completed. We now see a scatterplot of education level versus inference score. We can immediately see that as education level increases (as we move right on the plot), the number of blue points increases. So <span class="learn">the model is clearly learning that there is a positive correlation between education level and being high income</span>. Having higher education levels tend to lead to more specialized and better paying jobs, so it makes sense that the model has picked up on this pattern in the training data.</div>

            <div class="image-container"><img src="./assets/wit-eduvsscore.png" class="image"></div>

            <div class="mdl-typography--body-1 para">Facets Dive is incredibly flexible, and can create multiple interesting visualizations through its ability to bucket, scatter, and color datapoints. While the possibilities are endless, here is a small list of  visualizations that you may find interesting with this model.
              <div class="image-container"><img src="./assets/wit-smallmult.png" class="image"></div>
              <div class="mdl-typography--title"> Small Multiples Scatterplots | Age vs Inference Score for each Marital Status</div>
              Set the binning of X-axis by marital status, scattering of X-axis by age, scattering of Y-axis by inference score and color by inference label.
              <ul>
                <li class="mdl-typography--body-1"><span class="learn"> Married-civ-spouse in the most common marital status, followed by divorced and never married. The other categories have many less records.</span></li>
                <li class="mdl-typography--body-1"><span class="learn"> Married-civ-spouse seems to have the most balance between those predicted to have high income and those predicted to have low income. The other categories are heavily weighted towards predicting earning less.</span></li>
              </ul>
              <div class="image-container"><img src="./assets/wit-hours.png" class="image"></div>              
              <div class="mdl-typography--title">Histogram | Hours worked with Marital Sstatus indicated</div>
              Set the binning of the X-axis to hours-per-week. Remove the scatterplot settings by using "(default)". Color by marital status. 
              <ul>
                <li class="mdl-typography--body-1"><span class="learn">31-40 hours per week is the largest bin for hours worked in a week.</span></li>
                <li class="mdl-typography--body-1"><span class="learn"> There also seems to be a bump in the 50-59 bin. </span></li>
                <li class="mdl-typography--body-1"><span class="learn"> Some people reported working up to 99 hours a week.</span></li>
              </ul>
              <div class="image-container"><img src="./assets/wit-agemarital.png" class="image"></div>
              <div class="mdl-typography--title"> 2D Histograms | Age vs Marital Status with Inference Score indicated</div>
              Set the binning of the X-axis to age, the binning of the Y-axis to marital status, and color by inference label. 
              <ul>
                <li class="mdl-typography--body-1"><span class="learn">  The percentage of people who are never married goes down as age goes up and the percentage of people who are widowed goes up as age further increases. We see a correlation between the age and marital status features in the dataset.</span></li>
                <li class="mdl-typography--body-1"><span class="learn">  The percentage of people who are never married and fall in lower age brackets are more likely to be classified as low income by the model. </span></li>
              </ul>
            </div>

            <div class="mdl-typography--display-1">Viewing and editing the details of a datapoint</div>
            <div class="mdl-typography--body-1 para">Back on the initial view, let's further investigate a datapoint near the decision boundary, where datapoints turn from red to blue. Clicking on a datapoint highlights it in the visualization. The details of the datapoint should appear in the datapoint editor panel to the left of the visualization.</div>
            <div class="image-container"><img src="./assets/wit-selected.png" class="image"></div>
            <div class="mdl-typography--body-1 para">We can now see the specifics of the datapoint we clicked on, including its feature values and its inference results. <span class="learn">For this datapoint, the inference score for the positive (high income) class was 0.472, and the score for negative (low income) class was 0.528.</span> These scores are very close to the decision threshold of 0.5 that the tool initially uses. For a datapoint this close to the threshold, we could probably change one of many things about this datapoint to make the inference cross the threshold of 0.5. Let's try changing the age from 42 to 48 and clicking the "Run inference" button.</div>
            <div class="mdl-typography--body-1 para"> Notice that now there is a second "run" of results in the inference results section, in which the positive class score was 0.510. <span class="learn">By simply changing the age of this person, the model now predicts that they are high income.</span></div>
            <div class="image-container"><img src="./assets/wit-edited.png" class="image"></div>
            <div class="mdl-typography--display-1">Finding Nearest Counterfactuals</div>
            <div class="mdl-typography--body-1 para">Another way we can look at how changes to a person can cause changes in classification is by looking for a <a class="inline" href="https://arxiv.org/pdf/1711.00399.pdf">nearest counterfactual</a> to the selected datapoint. The nearest counterfactual is the most similar datapoint that has a different inference result. So, for this datapoint, which was inferred as being low income, the nearest counterfactual is the most similar person which the model inferred as being high income. WIT can find the nearest counterfactual using one of two ways to calculate similarity between datapoints, L1 and L2 distance. See <a class="inline" href="https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms">this page</a> for an explanation of the difference between these two distance measurements. Let's enable the counterfactual toggle button to find the nearest counterfactual using L1 distance. We now see two datapoints being compared side by side. The green text represents features where the two datapoints differ. <span class="learn">In this case, the nearest counterfactual is slightly older and has a different occupation, but is otherwise identical.</span></div>
            <div class="image-container"><img src="./assets/wit-cf.png" class="image"></div>
            <div class="mdl-typography--body-1 para">We can also see how similar every point in the dataset is to our selected datapoint through the "show similarity to selected datapoint" button. Here, the tool will calculate the distance from the selected datapoint to every other datapoint loaded, and will use that distance in the datapoints visualization. Let's change our X-axis scatter to show the L1 distance to the selected datapoint.</div>
            <div class="image-container"><img src="./assets/wit-distancedialog.png" class="image"></div>
            <div class="mdl-typography--body-1 para">Now our selected datapoint, highlighted in yellow, is the left-most datapoint as it is completely similar to itself. The points on the extreme right are the most dissimilar from the selected datapoint. And we can see the nearest counterfactual highlighted in green. Because it is the nearest counterfactual, is shows up as the left-most blue point in the visualization. If you have found one datapoint on which your model is doing something interesting/unexpected, this can be an interesting view to explore other similar datapoints in order to see how the model is performing on them.</div>
            <div class="image-container"><img src="./assets/wit-distance.png" class="image"></div>
            <div class="mdl-typography--display-1">Exploring partial dependence plots</div>
            <div class="mdl-typography--body-1 para">Partial dependence plots allow a principled approach to exploring how changes to a datapoint affect the model's prediction. Each partial dependence plot shows how the model's positive classification score changes as a single feature is adjusted in the datapoint. Upon clicking the "partial dependence plots" button in the right-side controls, we immediately see the plot for the selected datapoint if the age of this person is changed from a minimum of 17 to a maximum of 72.</div>
            <div class="mdl-typography--body-1 para">As the plot shows, as age increases, the model believes more confidently that this person is high income. The flat, semi-transparent line shows the current positive classification threshold being used, so points on the dark blue line above that threshold represent where the model would label someone as high-income. It seems that <span class="learn">the model has learned a positive correlation between age and income</span>, which makes sense as people tend to earn more money as they grow older. Of course, this assumption fails once people hit retirement later in life, but <span class="learn">a simple linear model doesn't contain the complexity to model this non-linear relationship between age and income.</span></div>
            <div class="image-container"><img src="./assets/wit-agepd.png" class="image"></div>
            <div class="mdl-typography--body-1 para">Clicking on the education header in the partial dependence plots area opens up the plot for changing the categorical (non-numeric) education feature. Not surprisingly, <span class="learn">more advanced degrees give the model more confidence in higher income.</span></div>
            <div class="image-container"><img src="./assets/wit-edupd.png" class="image"></div>
            <div class="mdl-typography--body-1 para">Another way to show the relationship between feature values and model scores is to look at global partial dependence plots. These each show how changing each feature individually affects all of the datapoints in the dataset. You can switch to global partial dependence plots through a switch button on the UI. Looking at the capital gains global partial dependence plot, we can see that on average, as capital gains crosses the 1000 mark, the positive classification score crosses above 0.5 and then skyrockets up towards 1.0. <span class="learn">High capital gains is a very strong indicator of high income, much more than any other single feature.</span></div>
            <div class="image-container"><img src="./assets/wit-capgainpd.png" class="image"></div>
            <div class="mdl-typography--display-1">Model Performance Analysis</div>
            <div class="mdl-typography--body-1 para">Now let's explore the Performance + Fairness tab of WIT, which allows us to look at overall model performance and ask questions about model performance across data slices. In order to investigate fairness, we need to tell the tool which feature in each datapoint the model is trying to predict, so the tool can see how well the model performs on each datapoint (does it get the prediction right or wrong?). In our case, that feature in the dataset is named "Over-50K", so we set the ground truth feature dropdown to the "Over-50K" feature.</div>
            <div class="mdl-typography--body-1 para">Now we can see a positive classification threshold slider, <a class="inline" href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>, and <a class="inline" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a> for the model. The confusion matrix shows, for the currently-set classification threshold, how many true positives, true negatives, false positives, and false negatives the model predicted over the dataset. The ROC curve shows the true positive rate and false positive rate for every possible setting of the positive classification threshold, with the current threshold called out as a highlighted point on the curve. <span class="learn">We can see that at the default threshold level of 0.5, our model is incorrect about 18% of the time, with about 10% of the time being false positives and 8% of the time being false negatives.</span></div>
            <div class="image-container"><img src="./assets/wit-perf.png" class="image"></div>
            <div class="mdl-typography--body-1 para">As we scrub the threshold down from its initial value of 0.5, we see the confusion matrix and ROC curve get updated. The confusion matrix shows that as the threshold is lowered, the model considers more and more datapoints as being high income (at a threshold value of 0.25, if the model predicts the positive class with a score of 0.25 or more, than the point is considered as being high income). This means more true positives and false positives, and less true negatives and false negatives. Also, the current threshold point on the ROC curve moves up and to the right, meaning a higher true positive rate and higher false positive rate, as the model becomes more permissive in who it deems as high income.</div>
            <div class="image-container"><img src="./assets/wit-lessperf.png" class="image"></div>
            <div class="mdl-typography--body-1 para">You'll notice in this tab, there is a setting for "cost ratio" and an "optimize threshold" button. WIT can automatically set the classification threshold to the optimal point, given the dataset loaded and the cost ratio, which specifies the relative cost of a false positive versus a false negative. This cost is something that a user needs to determine for themselves. The default cost ratio in the tool is 1, meaning false negatives and false positives are equally undesirable.</div>
            <div class="mdl-typography--display-1">Cost Ratios and Optimizing Decision Thresholds</div>
            <div class="mdl-typography--body-1 para">In some systems, such as a medical early screening test (where a positive classification would be an indication of a possible medical condition, requiring further medical testing), it is important to be more permissive with lower-scoring datapoints, preferring to predict more datapoints as in the positive class, at the risk of having more false positives (which would then be weeded out by the follow-up medical testing). In this case, we would want a low cost ratio, as we prefer false positives to false negatives. A cost ratio of 0.25 means that we consider a false negative 4 times as costly as a false positive.</div>
            <div class="mdl-typography--body-1 para">But in a system that unlocks a front door given a face identification match, it is more important to avoid false positives, as the expense of sometimes not incorrectly automatically opening the door for a complete stranger. In this case, we would want a high cost ratio, as we prefer false negatives to false positives. A cost ratio of 3 means that we consider a false positive 3 times as costly as a false negative.</div>
            <div class="mdl-typography--body-1 para">With the default cost ratio of 1, if we click "optimize threshold" then the positive classification threshold changes to 0.4. Optimizing for a cost ratio of 1 is the same as optimizing for accuracy, as it will minimize the total number of false positives and false negatives. In this case, <span class="learn">our simple linear model can be about 82% accurate over the dataset with the optimal threshold</span>. If deploying this binary classification model into a real application, this threshold setting might be ideal, assuming we don't wish to penalize false positives and false negatives differently. We say this might be a good setting, as that threshold setting should be verified over a larger test set if available, and there may be other factors to consider, such as fairness (which we will get into soon).</div>
            <div class="image-container"><img src="./assets/wit-opt.png" class="image"></div>
            <div class="mdl-typography--body-1 para">If we change the cost ratio to 2 and click the optimize threshold button, the optimal threshold moves up to 0.77. <span class="learn">the overall accuracy at this threshold is lower than at the previous threshold, but the number of false positives plummeted, as is desired with this cost ratio setting.</span></div>
            <div class="image-container"><img src="./assets/wit-opt2.png" class="image"></div>
            <div class="mdl-typography--display-1">ML Fairness</div>
            <div class="mdl-typography--body-1 para"><a class="inline" href="https://developers.google.com/machine-learning/fairness-overview/">Machine learning fairness</a> is an active and important area of research. Since ML models learn from labeled training data, their inferences will reflect the information contained inside the training data. Sometimes the data may come from a source that contains biases (human-labeled data containing the biases of those humans). Sometimes the data contains biases due to how it was collected (data only from users from a single country, for a product that will be deployed world-wide). There are countless ways that a dataset can be biased, leading to models trained from that dataset affecting different populations differently (such as a model giving less loans to women than men because it is based on historical, outdated data showing less women in the workplace).</div>
            <div class="mdl-typography--body-1 para">WIT can help investigate fairness concerns in a few different ways. First, the "Features" tab shows an overview of the provided dataset, using a visualization called <a class="inline" href="https://pair-code.github.io/facets/">Facets Overview</a>. Also, the tool can break down model performance by subsets of the data and look at fairness metrics between those subsets.</div>
            <div class="mdl-typography--title">Exploring Feature Overviews</div>
            <div class="image-container"><img src="./assets/wit-facets-overview.png" class="image"></div>
            <div class="mdl-typography--body-1 para">In the Features tab, we can look to see the distribution of values for each feature in the dataset. We can see that of the 5,000 test datapoints, over 3,300 are from men and over 4,200 are from caucasions. <span class="learn">Women and minorities seem under-represented in this dataset</span>. That may lead to the model not learning an accurate representation of the world in which it is trying to make predictions (of course, even if it does learn an accurate representation, is that what we want the model to perpetuate? This is a much deeper question still falling under the ML fairness umbrella and worthy of discussion outside of WIT). Predictions on those under-represented groups are more likely to be inaccurate than predictions on the over-represented groups.</div>
            <div class="mdl-typography--body-1 para">The features in this visualization can be sorted by a number of different metrics, including non-uniformity. With this sorting, the features that have the most non-uniform distributions are shown first. For numeric features, <span class="learn">capital gain is very non-uniform, with most datapoints having it set to 0, but a small number having non-zero capital gains, all the way up to a maximum of 100,000</span>. For categorical features, <span class="learn">country is the most non-uniform with most datapoints being from the USA, but there is a long tail of 40 other countries which are not well represented.</span></div>
            <div class="image-container"><img src="./assets/wit-non-uniform.png" class="image"></div>
            <div class="mdl-typography--title">Analyzing ML Fairness</div>
            <div class="mdl-typography--body-1 para">Back in the "Performance + Fairness" tab, we can set an input feature (or set of features) with which to slice the data. For example, setting this to "sex" allows us to see the breakdown of model performance on male datapoints versus female datapoints. We can see that <span class="learn">the model is more accurate (has less false positives and false negatives) on females than males</span>. We can also see that <span class="learn">the model predicts high income for females much less than it does for males (10% of the time for females vs 28% of the time for males).</span></div>
            <div class="mdl-typography--body-1 para">Imagine a scenario where this simple income classifier was used to approve or reject loan applications (not a realistic example but it illustrates the point). In this case, 28% of men from the test dataset have their loans approved but only 10% of women have theirs approved. If we wished to ensure than men and women get their loans approved the same percentage of the time, that is a fairness concept called "demographic parity". One way to achieve demographic parity would be to have different classification thresholds for males and females in our model. You'll notice there is a button on the tool labeled "demographic parity". When we press this button, the tool will take the cost ratio into account, and come up with ideal separate thresholds for men and women that will achieve demographic parity over the test dataset.</div>
            <div class="mdl-typography--body-1 para">In this case, <span class="learn">demographic parity can be found with both groups getting loans 16% of the time by having the male threshold at 0.78 and the female threshold at 0.12.</span> Because of the vast difference in the properties of the male and female training data in this 1994 census dataset, we need quite different thresholds to achieve demographic parity. Notice how <span class="learn">with the high male threshold there are many more false negatives than before, and with the low female threshold there are many more false positives than before.</span> This is necessary to get the percentage of positive predictions to be equal between the two groups. WIT has buttons to optimize for other fairness constraints as well, such as <a class="inline" href="http://research.google.com/bigpicture/attacking-discrimination-in-ml/">"equal opportunity"</a> and "equal accuracy".</div>
            <div class="image-container"><img src="./assets/wit-demoparity.png" class="image"></div>
            <div class="mdl-typography--body-1 para">The use of these features can help shed light on subsets of your data on which your classifier is performing very differently. Understanding biases in your datasets and data slices on which your model has disparate performance are very important parts of analyzing a model for fairness. There are many approaches to improving fairness, including augmenting training data, building fairness-related loss functions into your model training procedure, and post-training inference adjustments like those seen in WIT. We think that WIT provides a great interface for furthering ML fairness learning, but of course there is no silver bullet to improving ML fairness.</div>
            <hr>
            <div class="mdl-typography--display-1">Conclusion</div>
            <div class="mdl-typography--body-1 para">Thanks for checking out this walkthrough of the What-If Tool on the UCI census binary classification task. WIT has plenty of other features not included in this walkthrough, such as:</div>
            <div class="mdl-layout--title">Comparing the performance of two models.</div>
            <div class="mdl-typography--body-1">This <a class="inline" href="https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_Toxicity_Text_Model_Comparison.ipynb">notebook</a> shows how WIT can help us compare two models that predict toxicity of internet comments, one of which has had some de-biasing processing performed on it. It also shows how we can use WIT with non-TensorFlow models by providing a custom predict function for the tool to use.</div>
            <div class="mdl-typography--title">Analyzing models that take images as inputs</div> 
            <div class="mdl-typography--body-1"> This <a class="inline" href="https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_Smile_Detector.ipynb">notebook</a>demonstrates WIT on a smile detection classifier.</div>
            <div class="mdl-typography--body-1">Analyzing <a class="inline" href="https://medium.com/datadriveninvestor/regression-in-machine-learning-296caae933ec">regression</a> models </div>
            <div class="mdl-typography--body-1">In this <a class="inline" href="https://colab.research.google.com/github/PAIR-code/what-if-tool/blob/master/WIT_Age_Regression.ipynb">notebook</a>, we use the same UCI census dataset in order to predict people's ages from their census information.</div>
          </div>
        </div>
      </div>
    </div>
  </main>
</div>
</body>
</html>