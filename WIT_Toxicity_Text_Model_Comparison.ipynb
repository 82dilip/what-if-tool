{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIT Toxicity Text Model Comparison",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "UiNxsd4_q9wq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What-If Tool toxicity text model comparison\n",
        "\n",
        "Copyright 2019 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) to compare two text models that determine sentence toxicity, one of which has had some debiasing performed during training.\n",
        "\n",
        "This notebook loads two pretrained toxicity models from [ConversationAI](https://github.com/conversationai/unintended-ml-bias-analysis) and compares them on the [wikipedia comments dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973).\n",
        "\n",
        "This notebook also shows how the What-If Tool can be used on non-TensorFlow models. In this case, these models are keras models that do not use tensorflow Examples as an input format. By writing a simple wrapper for WitWidget, these models can be analyzed in the What-If Tool.\n",
        "\n",
        "##WARNING: Some text examples in this notebook include profanity, offensive statments, and offensive statments involving identity terms. Please feel free to avoid using this notebook.\n"
      ]
    },
    {
      "metadata": {
        "id": "qqB2tjOMETmr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Install latest TensorFlow and the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "# If running in colab then pip install, otherwise no need.\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade tf-nightly witwidget\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EBOHfrOP7Iy5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Download the pretrained keras model files\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_model.h5 -o ./cnn_wiki_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_hparams.h5 -o ./cnn_wiki_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_tokenizer.pkl -o ./cnn_wiki_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_model.h5 -o ./cnn_debias_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_hparams.h5 -o ./cnn_debias_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_tokenizer.pkl -o ./cnn_debias_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/wiki_test.csv -o ./wiki_test.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZR3i6UZlZ96",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Load the keras models\n",
        "from keras.models import load_model\n",
        "import cPickle as pkl\n",
        "\n",
        "model1 = load_model('cnn_wiki_tox_v3_model.h5')\n",
        "with open('cnn_wiki_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer1 = pkl.load(f)\n",
        "tokenizer1.oov_token = None # quick fix for version issues\n",
        "\n",
        "model2 = load_model('cnn_debias_tox_v3_model.h5')\n",
        "with open('cnn_debias_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer2 = pkl.load(f)\n",
        "tokenizer2.oov_token = None # quick fix for version issues"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nStoYhqT80WH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define custom prediction functions so that WIT infers using keras models\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set up model helper functions:\n",
        "PADDING_LEN = 250\n",
        "\n",
        "# Get raw string out of tf.Example and prepare it for keras model input\n",
        "def examples_to_model_in(examples, tokenizer):\n",
        "  texts = [ex.features.feature['comment'].bytes_list.value[0] for ex in examples]\n",
        "  # Tokenize string into fixed length sequence of integer based on tokenizer \n",
        "  # and model padding\n",
        "  text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "  model_ins = pad_sequences(text_sequences, maxlen=PADDING_LEN)\n",
        "  return model_ins\n",
        "\n",
        "# WIT predict functions:\n",
        "def custom_predict_1(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer1)\n",
        "  preds = model1.predict(model_ins)\n",
        "  return preds\n",
        "\n",
        "def custom_predict_2(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer2)\n",
        "  preds = model2.predict(model_ins)\n",
        "  return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXaUORW0DVhg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Define helper functions for dataset conversion from csv to tf.Examples\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "  examples = []\n",
        "  if columns == None:\n",
        "    columns = df.columns.values.tolist()\n",
        "  for index, row in df.iterrows():\n",
        "    example = tf.train.Example()\n",
        "    for col in columns:\n",
        "      if df[col].dtype is np.dtype(np.int64):\n",
        "        example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "      elif df[col].dtype is np.dtype(np.float64):\n",
        "        example.features.feature[col].float_list.value.append(row[col])\n",
        "      elif row[col] == row[col]:\n",
        "        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "    examples.append(example)\n",
        "  return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nu398ARdeuxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Read the dataset from CSV and process it for model {display-mode: \"form\"}\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'wiki_test.csv'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = None\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
        "df = df[['is_toxic', 'comment']]\n",
        "\n",
        "# Remove non ascii characters\n",
        "comments = df['comment'].values\n",
        "proc_comments = []\n",
        "for c in comments:\n",
        "  try:\n",
        "    proc_comments.append(c.decode('unicode_escape').encode('ascii', 'ignore').strip())\n",
        "  except:\n",
        "    proc_comments.append('')\n",
        "\n",
        "df = df.assign(comment=proc_comments)\n",
        "\n",
        "label_column = 'is_toxic'\n",
        "make_label_column_numeric(df, label_column, lambda val: val)\n",
        "\n",
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VNNEN1jRa2Vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To better see comment text in the What-If Tool, on the \"Datapoints\" tab, click the button pointed to by the red arrow in the image below.\n",
        "![](https://storage.googleapis.com/what-if-tool-resources/misc-resources/WIT-toxicity-comparison-colab-img1.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "UwiWGrLlSWGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Invoke What-If Tool for the data and two models (Note that this step may take a while due to prediction speed of the toxicity model){display-mode: \"form\"}\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "num_datapoints = 1000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 720  #@param {type: \"number\"}\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(\n",
        "  custom_predict_1).set_compare_custom_predict_fn(custom_predict_2)\n",
        "\n",
        "wv = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1s1_SiOyS0l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exploration ideas\n",
        "\n",
        "- Organize datapoints by setting X-axis scatter to \"inference score 1\" and Y-axis scatter to \"inference score 2\" to see how each datapoint differs in score between the original model (1) and debiased model (2). Points off the diagonal have differences in results between the two models.\n",
        "  - Are there patterns of which datapoints don't agree between the two models?\n",
        "  - If you set the ground truth feature dropdown in the \"Performance + Fairness\" tab to \"is_toxic\", then you can color or bin the datapoints by \"inference correct 1\" or \"inference correct 2\". Are there patterns of which datapoints are incorrect for model 1? For model 2?\n",
        "\n",
        "You may want to focus on terms listed [here](https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/unintended_ml_bias/bias_madlibs_data/adjectives_people.txt)"
      ]
    },
    {
      "metadata": {
        "id": "QuZjEn5qOHFH",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Add a feature column for each identity term to indicate if it exists in the comment\n",
        "!wget https://raw.githubusercontent.com/conversationai/unintended-ml-bias-analysis/master/unintended_ml_bias/bias_madlibs_data/adjectives_people.txt\n",
        "\n",
        "import re\n",
        "\n",
        "with open('adjectives_people.txt', 'r') as f:\n",
        "  segments = f.read().strip().split('\\n')\n",
        "print(segments)\n",
        "\n",
        "# Tag every sentence with an identity term\n",
        "comments = df['comment'].values\n",
        "seg_anns = {}\n",
        "selected_segments = segments\n",
        "for s in selected_segments:\n",
        "  is_seg = []\n",
        "  for c in comments:\n",
        "    if re.search(s, c):\n",
        "      is_seg.append(1)\n",
        "    else:\n",
        "      is_seg.append(0)\n",
        "  seg_anns[s] = is_seg\n",
        "\n",
        "for seg_key, seg_ann in seg_anns.iteritems():\n",
        "  df[seg_key] = pd.Series(seg_ann, index=df.index)\n",
        "\n",
        "examples = df_to_examples(df)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}